{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86025644-b9d1-4718-973b-7660d29649a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_script_from_file(file_path, dbname=\"synthea\", user=\"admin\", password=\"adminpassword\", host=\"localhost\", port=\"5432\"):\n",
    "    if not os.path.isfile(file_path):\n",
    "        return [False, f\"SQL file not found: {file_path}\"]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_script = f.read()\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=dbname,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            host=host,\n",
    "            port=port\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Executing SQL script for database '{dbname}' from file: {file_path}\")\n",
    "        cursor.execute(sql_script)\n",
    "        if cursor.description:\n",
    "            colnames = [desc[0] for desc in cursor.description]\n",
    "            rows = cursor.fetchall()\n",
    "            df = pd.DataFrame(rows, columns=colnames)\n",
    "            return [True, df.to_string()]\n",
    "        \n",
    "        return [True, \"SQL executed successfully with no results to display.\"]\n",
    "    except Exception as e:\n",
    "        return [False, f\"SQL Execution Error: {str(e)}\"]\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        error_msg = f\"[{dbname}] SQL Execution Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return [False, error_msg]\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"[{dbname}] Unexpected Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return [False, error_msg]\n",
    "\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "TOGETHER_API_KEY = \"bdd4a08692807d05983219eec866289d342941595cb1dd726d2ec128404dd4d1\"\n",
    "\n",
    "def together_chat_completion_stream_clean(prompt, model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"):\n",
    "    if not TOGETHER_API_KEY:\n",
    "        raise ValueError(\"La cl√© API Together AI n'est pas configur√©e.\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "    url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    response_text = \"\"\n",
    "    with requests.post(url, headers=headers, json=data, stream=True) as response:\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Erreur API: {response.status_code} - {response.text}\")\n",
    "        \n",
    "        for chunk in response.iter_lines():\n",
    "            if chunk:\n",
    "                decoded_chunk = chunk.decode('utf-8')\n",
    "                if decoded_chunk.startswith(\"data:\"):\n",
    "                    try:\n",
    "                        parsed = json.loads(decoded_chunk[5:].strip())\n",
    "                        content = parsed.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\")\n",
    "                        if content:\n",
    "                            print(content, end='', flush=True)\n",
    "                            response_text += content\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "    \n",
    "    return response_text.strip()\n",
    "\n",
    "def extract_and_append_sql(text, json_data, sql_file_path):\n",
    "    # Pattern corrig√© : \\s* au lieu de \\s+ pour accepter z√©ro ou plusieurs espaces/nouvelles lignes\n",
    "    sql_blocks = re.findall(r\"```sql\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Si le premier pattern ne fonctionne pas, essayer sans 'sql'\n",
    "    if not sql_blocks:\n",
    "        sql_blocks = re.findall(r\"```\\s*sql\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Dernier essai avec n'importe quelles balises ```\n",
    "    if not sql_blocks:\n",
    "        sql_blocks = re.findall(r\"```[a-zA-Z]*\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if not sql_blocks:\n",
    "        print(f\"‚ùå Aucun bloc SQL trouv√© pour l'√©tape {json_data.get('id', 'unknown')}\")\n",
    "        print(f\"Texte analys√© (premiers 200 chars): {text[:200]}\")\n",
    "        return\n",
    "    \n",
    "    step_id = json_data.get(\"id\", \"unknown\")\n",
    "    \n",
    "    # Cr√©er le dossier s'il n'existe pas\n",
    "    os.makedirs(os.path.dirname(sql_file_path), exist_ok=True)\n",
    "    \n",
    "    with open(sql_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for block in sql_blocks:\n",
    "            f.write(f\"-- BEGIN STEP: {step_id}\\n\")\n",
    "            f.write(block.strip() + \"\\n\")\n",
    "            f.write(f\"-- END STEP: {step_id}\\n\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Ajout√© {len(sql_blocks)} bloc(s) SQL pour l'√©tape {step_id} dans {sql_file_path}\")\n",
    "\n",
    "def edit_sql_file(id: int, sql_script: str, file_named: str):\n",
    "    begin_marker = f\"-- BEGIN STEP: {id}\"\n",
    "    end_marker = f\"-- END STEP: {id}\"\n",
    "\n",
    "    with open(file_named, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    start_index = None\n",
    "    end_index = None\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == begin_marker:\n",
    "            start_index = i\n",
    "        elif line.strip() == end_marker:\n",
    "            end_index = i\n",
    "            break\n",
    "\n",
    "    if start_index is None or end_index is None or start_index >= end_index:\n",
    "        raise ValueError(f\"STEP with id {id} not found in the file.\")\n",
    "\n",
    "    # Replace the lines between the markers with the new SQL script\n",
    "    new_script_lines = [begin_marker + '\\n'] + [line + '\\n' for line in sql_script.strip().split('\\n')] + [end_marker + '\\n']\n",
    "    lines = lines[:start_index] + new_script_lines + lines[end_index + 1:]\n",
    "\n",
    "    with open(file_named, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "def contains_task_completed(text):\n",
    "    return re.search(r\"task\\s+completed\", text, re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3106933-22c4-4240-a01a-51ca901bcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```sql\n",
      "-- Section 0 start\n",
      "\n",
      "CREATE VIEW location_enriched_view AS\n",
      "SELECT \n",
      "  T1.city,\n",
      "  T1.state,\n",
      "  T1.zip,\n",
      "  T2.state_abbreviation\n",
      "FROM \n",
      "  synthea_source.patients T1\n",
      "  LEFT JOIN omop.state_map T2\n",
      "    ON T1.state = T2.state;\n",
      "\n",
      "-- Section 0 end\n",
      "```‚úÖ Ajout√© 1 bloc(s) SQL pour l'√©tape 0 dans SQL/location.sql\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Erreur API: 429 - {\n  \"id\": \"nw7UT25-2j9zxn-947e30ea8e90d166\",\n  \"error\": {\n    \"message\": \"You have reached the rate limit specific to this model meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo. The maximum rate limit for this model is 6.0 queries and 180000000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\",\n    \"type\": \"model_rate_limit\",\n    \"param\": null,\n    \"code\": null\n  }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     33\u001b[39m conversation_history.append({\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHere is the JSON:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mjson_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m })\n\u001b[32m     38\u001b[39m full_context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([m[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m conversation_history])\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m llm_response = \u001b[43mtogether_chat_completion_stream_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m extract_and_append_sql(llm_response, json_data, \u001b[33m\"\u001b[39m\u001b[33mSQL/location.sql\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m conversation_history.append({\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: llm_response\n\u001b[32m     46\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mtogether_chat_completion_stream_clean\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m requests.post(url, headers=headers, json=data, stream=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mErreur API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response.iter_lines():\n\u001b[32m     73\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m chunk:\n",
      "\u001b[31mException\u001b[39m: Erreur API: 429 - {\n  \"id\": \"nw7UT25-2j9zxn-947e30ea8e90d166\",\n  \"error\": {\n    \"message\": \"You have reached the rate limit specific to this model meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo. The maximum rate limit for this model is 6.0 queries and 180000000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\",\n    \"type\": \"model_rate_limit\",\n    \"param\": null,\n    \"code\": null\n  }\n}"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os \n",
    "import time \n",
    "from unitest_OMOP import unittest_location_table\n",
    "\n",
    "folder = \"json/location/\"\n",
    "conversation_history = []\n",
    "\n",
    "with open(\"prompt/prompt1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt\n",
    "}\n",
    "\n",
    "conversation_history.append(system_message)\n",
    "\n",
    "for idx, file_name in enumerate(sorted(os.listdir(folder))):\n",
    "    path = os.path.join(folder, file_name)\n",
    "    if not os.path.isfile(path):\n",
    "        continue\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    json_str = json.dumps(json_data, indent=2)\n",
    "\n",
    "    # FIX: Only add JSON once, not twice\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Here is the JSON:\\n\\n{json_str}\"\n",
    "    })\n",
    "\n",
    "    full_context = \"\\n\\n\".join([m[\"content\"] for m in conversation_history])\n",
    "    llm_response = together_chat_completion_stream_clean(full_context, model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\")\n",
    "\n",
    "    extract_and_append_sql(llm_response, json_data, \"SQL/location.sql\")\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": llm_response\n",
    "    })\n",
    "\n",
    "with open(\"conversation_history.json\", \"w\") as f:\n",
    "    json.dump(conversation_history, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e36f6-2cd2-405f-86b6-c5f4dc9f52a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "239d3f50-f620-4c62-b495-f2b74cf5c598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SQL script for database 'synthea' from file: SQL/location.sql\n",
      "```sql\n",
      "-- STEP 1: omop.location\n",
      "\n",
      "INSERT INTO omop.location (\n",
      "    location_id,\n",
      "    city,\n",
      "    state,\n",
      "    zip,\n",
      "    location_source_value,\n",
      "    address_1,\n",
      "    address_2,\n",
      "    county\n",
      ")\n",
      "SELECT \n",
      "    MD5((city || state_abbreviation || zip)::text) AS location_id,\n",
      "    city,\n",
      "    state_abbreviation AS state,\n",
      "    zip,\n",
      "    zip AS location_source_value,\n",
      "    NULL::VARCHAR AS address_1,\n",
      "    NULL::VARCHAR AS address_2,\n",
      "    NULL::VARCHAR AS county\n",
      "FROM \n",
      "    (\n",
      "    SELECT \n",
      "        city,\n",
      "        state_abbreviation,\n",
      "        zip,\n",
      "        ROW_NUMBER() OVER (PARTITION BY city, state_abbreviation, zip ORDER BY city) AS row_num\n",
      "    FROM \n",
      "        omop.location_enriched_view\n",
      "    ) AS subquery\n",
      "WHERE \n",
      "    row_num = 1;\n",
      "\n",
      "-- END STEP 1: omop.location\n",
      "```Executing SQL script for database 'synthea' from file: SQL/location.sql\n",
      "```sql\n",
      "-- STEP 1: omop.location\n",
      "\n",
      "INSERT INTO omop.location (\n",
      "    location_id,\n",
      "    city,\n",
      "    state,\n",
      "    zip,\n",
      "    location_source_value,\n",
      "    address_1,\n",
      "    address_2,\n",
      "    county\n",
      ")\n",
      "SELECT \n",
      "    MD5((city || state_abbreviation || zip)::text) AS location_id,\n",
      "    city,\n",
      "    state_abbreviation AS state,\n",
      "    zip,\n",
      "    zip AS location_source_value,\n",
      "    NULL::VARCHAR AS address_1,\n",
      "    NULL::VARCHAR AS address_2,\n",
      "    NULL::VARCHAR AS county\n",
      "FROM \n",
      "    (\n",
      "    SELECT \n",
      "        city,\n",
      "        state_abbreviation,\n",
      "        zip,\n",
      "        ROW_NUMBER() OVER (PARTITION BY city, state_abbreviation, zip ORDER BY city) AS row_num\n",
      "    FROM \n",
      "        (\n",
      "        SELECT DISTINCT \n",
      "            city,\n",
      "            state_abbreviation,\n",
      "            zip\n",
      "        FROM \n",
      "            omop.location_enriched_view\n",
      "        ) AS subquery\n",
      "    ) AS subquery\n",
      "WHERE \n",
      "    row_num = 1;\n",
      "\n",
      "-- END STEP 1: omop.location\n",
      "```Executing SQL script for database 'synthea' from file: SQL/location.sql\n",
      "```sql\n",
      "-- STEP 1: omop.location\n",
      "\n",
      "INSERT INTO omop.location (\n",
      "    location_id,\n",
      "    city,\n",
      "    state,\n",
      "    zip,\n",
      "    location_source_value,\n",
      "    address_1,\n",
      "    address_2,\n",
      "    county\n",
      ")\n",
      "SELECT \n",
      "    MD5((city || state_abbreviation || zip)::text) AS location_id,\n",
      "    city,\n",
      "    state_abbreviation AS state,\n",
      "    zip,\n",
      "    zip AS location_source_value,\n",
      "    NULL::VARCHAR AS address_1,\n",
      "    NULL::VARCHAR AS address_2,\n",
      "    NULL::VARCHAR AS county\n",
      "FROM \n",
      "    (\n",
      "    SELECT DISTINCT \n",
      "        city,\n",
      "        state_abbreviation,\n",
      "        zip\n",
      "    FROM \n",
      "        omop.location_enriched_view\n",
      "    ) AS subquery;\n",
      "\n",
      "-- END STEP 1: omop.location\n",
      "```\n",
      "Processed 11 messages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i=0\n",
    "\n",
    "def cleanup_location_table(dbname=\"synthea\"):\n",
    "    \"\"\"Nettoie la table location avant r√©ex√©cution\"\"\"\n",
    "    cleanup_sql = \"\"\"\n",
    "    DROP VIEW IF EXISTS omop.location_enriched_view;\n",
    "    DELETE FROM omop.location;\n",
    "    \"\"\"\n",
    "    return execute_sql_script_from_file(cleanup_sql,dbname=\"synthea\")\n",
    "\n",
    "\n",
    "while i < 3 :\n",
    "    cleanup_location_table(\"synthea\")\n",
    "    result = execute_sql_script_from_file(\"SQL/location.sql\", dbname=\"synthea\")\n",
    "\n",
    "    if result[0]:\n",
    "        success_msg = f\"# ‚úÖ SQL executed successfully.\\n\\n{result[1]}\" \n",
    "        test_msg = unittest_location_table(user=\"admin\", password=\"adminpassword\", host=\"localhost\", port=\"5432\", database=\"synthea\")\n",
    "        conversation_history.append({\n",
    "            \"role\": \"ipython\",\n",
    "            \"content\": success_msg +\"\\n\\n\"+ test_msg\n",
    "        })\n",
    "    else:\n",
    "        error_msg = f\"# ‚ùå SQL Execution Error:\\n{result[1]}\"\n",
    "        conversation_history.append({\n",
    "            \"role\": \"ipython\",\n",
    "            \"content\": error_msg\n",
    "        })\n",
    "\n",
    "    full_context = \"\\n\\n\".join([m[\"content\"] for m in conversation_history])\n",
    "    llm_response = together_chat_completion_stream_clean(full_context, model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\")\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": llm_response\n",
    "    })\n",
    "    \n",
    "    pattern = r'<function=([a-zA-Z_]+)>({.*?})<\\/function>'\n",
    "    \n",
    "    # FIX: Process function calls properly\n",
    "    function_calls_found = False\n",
    "    for match in re.finditer(pattern, llm_response, re.DOTALL):\n",
    "        function_calls_found = True\n",
    "        function_name = match.group(1)\n",
    "        function_args_raw = match.group(2)\n",
    "        \n",
    "        try:\n",
    "            function_args = json.loads(function_args_raw)\n",
    "            \n",
    "            if function_name == \"edit_sql_file\":\n",
    "                sql_script = function_args.get(\"sql_script\")\n",
    "                id = function_args.get(\"id\")\n",
    "                file_named = function_args.get(\"file_named\")\n",
    "                edit_sql_file(id, sql_script, file_named)\n",
    "                print(f\"Edited SQL for step {id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing function call: {e}\")\n",
    "        \n",
    "    if contains_task_completed(llm_response):\n",
    "        print(\"‚úÖ Task completed.\")\n",
    "        break\n",
    "    i = i +1 \n",
    "\n",
    "with open(\"conversation_history.json\", \"w\") as f:\n",
    "    json.dump(conversation_history, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessed {len(conversation_history)} messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5241f2-98dc-4ca3-be1a-fb06d525c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ca6776-d013-4baf-9742-184c83d3a147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the generated SQL code:\n",
      "\n",
      "```\n",
      "INSERT INTO omop.location (\n",
      "    city,\n",
      "    state,\n",
      "    zip,\n",
      "    location_source_value,\n",
      "    address_1,\n",
      "    address_2,\n",
      "    county\n",
      ")\n",
      "SELECT \n",
      "    l.city,\n",
      "    l.state_abbreviation AS state,\n",
      "    l.zip,\n",
      "    l.zip AS location_source_value,\n",
      "    NULL AS address_1, -- since column-level transformation rule is to null\n",
      "    NULL AS address_2, -- since column-level transformation rule is to null\n",
      "    NULL AS county   -- since column-level transformation rule is to null\n",
      "FROM \n",
      "    location_enriched_view AS l;\n",
      "```\n",
      "\n",
      "Note: I've included inline comments to clarify the logic of column-level transformation rules that result in `NULL` values. I've also assumed that the `cast_type` values for `address_1`, `address_2`, and `county` are not necessary since they don't seem to have a specific transformation rule. If there's an error in the execution, I'd be happy to review and correct!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"prompt/prompt1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "try:\n",
    "\n",
    "    with open(\"json/location/location_test_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    json_str = json.dumps(json_data, indent=2)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"JSON file not found. Please check the file path.\")\n",
    "    json_str = \"{}\"\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Invalid JSON format: {e}\")\n",
    "    json_str = \"{}\"\n",
    "\n",
    "final_prompt = prompt + \"\\n\\n\" + json_str\n",
    "result = groq_chat_completion_stream_clean(final_prompt)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ee806-e07a-4db8-9c26-b1984501571a",
   "metadata": {},
   "source": [
    "Here is the generated SQL code:\n",
    "\n",
    "```\n",
    "CREATE OR REPLACE VIEW omop.location_enriched_view AS \n",
    "SELECT \n",
    "  s.city, \n",
    "  s.state, \n",
    "  s.zip, \n",
    "  sm.state_abbreviation\n",
    "FROM \n",
    "  synthea.patients s \n",
    "  LEFT JOIN omop.states_map ON s.state = sm.state;\n",
    "```\n",
    "```\n",
    "INSERT INTO omop.location (\n",
    "  location_id,\n",
    "  city,\n",
    "  state,\n",
    "  zip,\n",
    "  location_source_value,\n",
    "  address_1,\n",
    "  address_2,\n",
    "  county\n",
    ")\n",
    "SELECT \n",
    "  MD5HASH(city || state_abbreviation || zip)::uuid AS location_id,\n",
    "  city,\n",
    "  state_abbreviation AS state,\n",
    "  zip,\n",
    "  zip AS location_source_value,\n",
    "  NULL::VARCHAR AS address_1,\n",
    "  NULL::VARCHAR AS address_2,\n",
    "  NULL::VARCHAR AS county\n",
    "FROM \n",
    "  location_enriched_view;\n",
    "```\n",
    "Let me know if this generates any execution errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2915c9df-a207-4c9b-8a84-a2e2a04e4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìã ANALYSE DE CONVERSATION\n",
      "================================================================================\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "ü§ñ SYST√àME - Message 1\n",
      "------------------------------------------------------------\n",
      "üìã Instructions syst√®me:\n",
      "   You are a highly capable AI specialized in generating optimized SQL code.\n",
      "\n",
      "üéØ Objective:\n",
      "   Generate SQL code that transforms data from a **source database (A)** into a **target database (B)** named `omop`.\n",
      "   Each input will include one or more JSON objects describing the transformation logic for a **single specific target table**.\n",
      "\n",
      "üéØ Input Details:\n",
      "   Each JSON file will contain:\n",
      "  ‚Ä¢ The structure of the source table(s) involved\n",
      "  ‚Ä¢ Column-level transformation rules\n",
      "  ‚Ä¢ Optional metadata (e.g. view names, output type, primary key logic)\n",
      "\n",
      "üéØ Constraints:\n",
      "  ‚Ä¢ Only generate SQL for the specific target table defined in the JSON.\n",
      "  ‚Ä¢ Do not infer or process transformations for any other target tables.\n",
      "  ‚Ä¢ All SQL must be limited to transforming data for the **one target table** described.\n",
      "\n",
      "üéØ Instructions:\n",
      "  ‚Ä¢ Carefully analyze the input JSON to understand the transformation logic.\n",
      "  ‚Ä¢ Generate clean, well-structured, and **readable PostgreSQL SQL code**.\n",
      "  ‚Ä¢ Use the format: `INSERT INTO omop.target_table (...) SELECT ...`\n",
      "  ‚Ä¢ Include inline SQL comments **only** when necessary to clarify non-obvious logic.\n",
      "  ‚Ä¢ Do **not** include any narrative explanation or commentary in your output.\n",
      "\n",
      "üéØ Output:\n",
      "  ‚Ä¢ Output **only** the final SQL code needed to transform and insert data into the specified `omop` table.\n",
      "\n",
      "üéØ Execution:\n",
      "   All generated SQL code will be merged and tested after your completion.\n",
      "   If any execution errors occur, they will be returned via the IPython role for correction.\n",
      "\n",
      "------------------------------------------------------------\n",
      "üë§ UTILISATEUR - Message 2\n",
      "------------------------------------------------------------\n",
      "üí¨ Demande utilisateur:\n",
      "üìù Here is the\n",
      "\n",
      "üìÑ Donn√©es JSON fournies:\n",
      "  id: 0\n",
      "  output_type: view\n",
      "  view_name: location_enriched_view\n",
      "  source_table: synthea.patients\n",
      "  joins: list avec 1 √©l√©ments\n",
      "  select: list avec 4 √©l√©ments\n",
      "\n",
      "------------------------------------------------------------\n",
      "üîß ASSISTANT - Message 3\n",
      "------------------------------------------------------------\n",
      "üîß R√©ponse assistant:\n",
      "üíæ Code SQL g√©n√©r√©:\n",
      "Here is the generated SQL code:\n",
      "\n",
      "-- \n",
      "INSERT INTO omop.location_enriched_view \n",
      "(\n",
      "    city, \n",
      "    state, \n",
      "    zip, \n",
      "    state_abbreviation\n",
      ")\n",
      "SELECT \n",
      "    p.city, \n",
      "    p.state, \n",
      "    p.zip, \n",
      "    sm.state_abbreviation\n",
      "FROM \n",
      "    synthea.patients p\n",
      "LEFT JOIN \n",
      "    omop.states_map sm \n",
      "ON \n",
      "    p.state = sm.state\n",
      "\n",
      "------------------------------------------------------------\n",
      "üë§ UTILISATEUR - Message 4\n",
      "------------------------------------------------------------\n",
      "üí¨ Demande utilisateur:\n",
      "üìù Here is the\n",
      "\n",
      "üìÑ Donn√©es JSON fournies:\n",
      "  id: 1\n",
      "  target_table: omop.location\n",
      "  source_table: None\n",
      "  source_view: location_enriched_view\n",
      "  output_type: insert\n",
      "  primary_key: dict avec 4 √©l√©ments\n",
      "  columns: list avec 7 √©l√©ments\n",
      "\n",
      "------------------------------------------------------------\n",
      "üîß ASSISTANT - Message 5\n",
      "------------------------------------------------------------\n",
      "üîß R√©ponse assistant:\n",
      "üíæ Code SQL g√©n√©r√©:\n",
      "Here is the generated SQL code:\n",
      "\n",
      "INSERT INTO omop.location \n",
      "(\n",
      "    location_id, \n",
      "    city, \n",
      "    state, \n",
      "    zip, \n",
      "    location_source_value, \n",
      "    address_1, \n",
      "    address_2, \n",
      "    county\n",
      ")\n",
      "SELECT \n",
      "    MD5.Hash(MD5.Hash(l.city || l.state_abbreviation || l.zip))::text AS location_id, \n",
      "    l.city, \n",
      "    l.state, \n",
      "    l.zip, \n",
      "    l.zip, \n",
      "    NULL::VARCHAR, \n",
      "    NULL::VARCHAR, \n",
      "    NULL::VARCHAR\n",
      "FROM \n",
      "    (SELECT \n",
      "        city, \n",
      "        state, \n",
      "        zip, \n",
      "        state_abbreviation\n",
      "    FROM \n",
      "        location_enriched_view) l \n",
      "\n",
      "Here is the explanation:\n",
      "\n",
      "* The primary key is generated using a hash function based on the combination of city, state_abbreviation, and zip.\n",
      "* The columns city, state, zip, and location_source_value are mapped directly from the source columns.\n",
      "* The columns address_1, address_2, and county are set to NULL as per the provided transformation rules.\n",
      "\n",
      "------------------------------------------------------------\n",
      "‚ö° EX√âCUTION - Message 6\n",
      "------------------------------------------------------------\n",
      "‚ö° R√©sultat d'ex√©cution:\n",
      "‚ùå # ‚ùå SQL Execution Error:\n",
      "[location] SQL Execution Error: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: FATAL:  database \"location\" does not exist\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "üîß ASSISTANT - Message 7\n",
      "------------------------------------------------------------\n",
      "üîß R√©ponse assistant:\n",
      "üíæ Code SQL g√©n√©r√©:\n",
      "I apologize for the error. It seems that the generated SQL code does not take into account the fact that the `omop.location` table may not exist in the target database. \n",
      "\n",
      "I will make sure to add a check for the existence of the table before executing the insertion query. If the table does not exist, it will create the table first.\n",
      "\n",
      "Here is the revised SQL code:\n",
      "\n",
      "```\n",
      "BEGIN\n",
      "    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name='location' AND table_schema='public') THEN\n",
      "        CREATE TABLE omop.location (\n",
      "            location_id VARCHAR(255) PRIMARY KEY,\n",
      "            city VARCHAR(255),\n",
      "            state VARCHAR(255),\n",
      "            zip VARCHAR(255),\n",
      "            location_source_value VARCHAR(255),\n",
      "            address_1 VARCHAR(255),\n",
      "            address_2 VARCHAR(255),\n",
      "            county VARCHAR(255)\n",
      "        );\n",
      "    END IF;\n",
      "\n",
      "    INSERT INTO omop.location \n",
      "    (\n",
      "        location_id, \n",
      "        city, \n",
      "        state, \n",
      "        zip, \n",
      "        location_source_value, \n",
      "        address_1, \n",
      "        address_2, \n",
      "        county\n",
      "    )\n",
      "    SELECT \n",
      "        MD5.Hash(MD5.Hash(l.city || l.state_abbreviation || l.zip))::text AS location_id, \n",
      "        l.city, \n",
      "        l.state, \n",
      "        l.zip, \n",
      "        l.zip, \n",
      "        NULL::VARCHAR, \n",
      "        NULL::VARCHAR, \n",
      "        NULL::VARCHAR\n",
      "    FROM \n",
      "        (SELECT \n",
      "            city, \n",
      "            state, \n",
      "            zip, \n",
      "            state_abbreviation\n",
      "        FROM \n",
      "            location_enriched_view) l \n",
      "END;\n",
      "```\n",
      "\n",
      "This revised SQL code will first check if the `omop.location` table exists. If it does not exist, it will create the table. If the table already exists, it will proceed with the insertion query.\n",
      "\n",
      "------------------------------------------------------------\n",
      "‚ö° EX√âCUTION - Message 8\n",
      "------------------------------------------------------------\n",
      "‚ö° R√©sultat d'ex√©cution:\n",
      "‚ùå # ‚ùå SQL Execution Error:\n",
      "[location] SQL Execution Error: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: FATAL:  database \"location\" does not exist\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "üîß ASSISTANT - Message 9\n",
      "------------------------------------------------------------\n",
      "üîß R√©ponse assistant:\n",
      "üíæ Code SQL g√©n√©r√©:\n",
      "It seems you're encountering an error with the connection to the server. I'll revise the SQL code again to address this. It appears that the database \"location\" does not exist in the target database, which was not mentioned in the input JSON.\n",
      "\n",
      "------------------------------------------------------------\n",
      "‚ö° EX√âCUTION - Message 10\n",
      "------------------------------------------------------------\n",
      "‚ö° R√©sultat d'ex√©cution:\n",
      "‚ùå # ‚ùå SQL Execution Error:\n",
      "[location] SQL Execution Error: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: FATAL:  database \"location\" does not exist\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "üîß ASSISTANT - Message 11\n",
      "------------------------------------------------------------\n",
      "üîß R√©ponse assistant:\n",
      "üíæ Code SQL g√©n√©r√©:\n",
      "You're having some connection issues. Let me take another look at the JSON inputs again. Ah-ha! I see that the JSON doesn't specify the target database name, but rather the target table name. That's why we're getting a database \"location\" not found error. \n",
      "\n",
      "Let's revise the SQL code to use the omop database. I also noticed that the primary key generation logic might need some adjustments. Currently, it's generating a hash based on the combination of city, state_abbreviation, and zip. If this is indeed the intended logic, we might want to ensure that the same combination of values does not result in the same primary key. We can do this by adding a unique constraint on these columns.\n",
      "\n",
      "Here's the revised SQL code:\n",
      "\n",
      "```sql\n",
      "BEGIN\n",
      "    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name='location' AND table_schema='public' AND table_catalog='omop') THEN\n",
      "        CREATE TABLE omop.location (\n",
      "            location_id VARCHAR(255) PRIMARY KEY,\n",
      "            city VARCHAR(255),\n",
      "            state VARCHAR(255),\n",
      "            zip VARCHAR(255),\n",
      "            location_source_value VARCHAR(255),\n",
      "            address_1 VARCHAR(255),\n",
      "            address_2 VARCHAR(255),\n",
      "            county VARCHAR(255),\n",
      "            UNIQUE (city, state, zip)\n",
      "        );\n",
      "    END IF;\n",
      "\n",
      "    INSERT INTO omop.location \n",
      "    (\n",
      "        location_id, \n",
      "        city, \n",
      "        state, \n",
      "        zip, \n",
      "        location_source_value, \n",
      "        address_1, \n",
      "        address_2, \n",
      "        county\n",
      "    )\n",
      "    SELECT \n",
      "        to_hex(replace(md5(replace('city ' || state_abbreviation || zip, ' ', '')), '/', '')::bytea)::varchar(255) AS location_id, \n",
      "        city, \n",
      "        state, \n",
      "        zip, \n",
      "        zip, \n",
      "        NULL::VARCHAR, \n",
      "        NULL::VARCHAR, \n",
      "        NULL::VARCHAR\n",
      "    FROM \n",
      "        (SELECT \n",
      "            city, \n",
      "            state, \n",
      "            zip, \n",
      "            state_abbreviation\n",
      "        FROM \n",
      "            location_enriched_view) l \n",
      "END;\n",
      "```\n",
      "\n",
      "This revised SQL code creates the `omop.location` table in the `omop` database, if it does not exist. It also sets the city, state, and zip columns as UNIQUE, ensuring that combinations of these values are not duplicated. This will avoid having the same primary key generated for different combinations.\n",
      "\n",
      "================================================================================\n",
      "üìä R√âSUM√â: 11 messages analys√©s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_conversation(json_input):\n",
    "    \"\"\"\n",
    "    Transforme une conversation JSON complexe en format lisible\n",
    "    \n",
    "    Args:\n",
    "        json_input: peut √™tre:\n",
    "            - Un chemin vers un fichier JSON\n",
    "            - Un string JSON\n",
    "            - Un dict ou une liste de messages\n",
    "    \n",
    "    Returns:\n",
    "        String format√© de la conversation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Si c'est un chemin de fichier\n",
    "    if isinstance(json_input, str) and (json_input.endswith('.json') or '/' in json_input or '\\\\' in json_input):\n",
    "        try:\n",
    "            with open(json_input, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            return f\"‚ùå Erreur: Fichier '{json_input}' non trouv√©\"\n",
    "        except json.JSONDecodeError as e:\n",
    "            return f\"‚ùå Erreur: JSON invalide dans le fichier '{json_input}'\\nD√©tail: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Erreur lors de la lecture du fichier '{json_input}': {e}\"\n",
    "    \n",
    "    # Si c'est un string JSON\n",
    "    elif isinstance(json_input, str):\n",
    "        try:\n",
    "            data = json.loads(json_input)\n",
    "        except json.JSONDecodeError:\n",
    "            return \"‚ùå Erreur: JSON invalide\"\n",
    "    else:\n",
    "        data = json_input\n",
    "    \n",
    "    # Si ce n'est pas une liste, on l'encapsule\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    # Mapping des r√¥les vers des symboles/couleurs\n",
    "    role_symbols = {\n",
    "        'system': 'ü§ñ SYST√àME',\n",
    "        'user': 'üë§ UTILISATEUR', \n",
    "        'assistant': 'üîß ASSISTANT',\n",
    "        'ipython': '‚ö° EX√âCUTION'\n",
    "    }\n",
    "    \n",
    "    conversation = []\n",
    "    conversation.append(\"=\" * 80)\n",
    "    conversation.append(\"üìã ANALYSE DE CONVERSATION\")\n",
    "    conversation.append(\"=\" * 80)\n",
    "    conversation.append(\"\")\n",
    "    \n",
    "    for i, message in enumerate(data, 1):\n",
    "        if not isinstance(message, dict) or 'role' not in message:\n",
    "            conversation.append(f\"‚ùå Message {i}: Format invalide\")\n",
    "            continue\n",
    "            \n",
    "        role = message.get('role', 'unknown')\n",
    "        content = message.get('content', '[Pas de contenu]')\n",
    "        \n",
    "        # En-t√™te du message\n",
    "        symbol = role_symbols.get(role, f'‚ùì {role.upper()}')\n",
    "        conversation.append(f\"\\n{'-' * 60}\")\n",
    "        conversation.append(f\"{symbol} - Message {i}\")\n",
    "        conversation.append(f\"{'-' * 60}\")\n",
    "        \n",
    "        # Traitement sp√©cialis√© selon le r√¥le\n",
    "        if role == 'system':\n",
    "            conversation.append(\"üìã Instructions syst√®me:\")\n",
    "            conversation.append(format_system_content(content))\n",
    "            \n",
    "        elif role == 'user':\n",
    "            conversation.append(\"üí¨ Demande utilisateur:\")\n",
    "            conversation.append(format_user_content(content))\n",
    "            \n",
    "        elif role == 'assistant':\n",
    "            conversation.append(\"üîß R√©ponse assistant:\")\n",
    "            conversation.append(format_assistant_content(content))\n",
    "            \n",
    "        elif role == 'ipython':\n",
    "            conversation.append(\"‚ö° R√©sultat d'ex√©cution:\")\n",
    "            conversation.append(format_ipython_content(content))\n",
    "            \n",
    "        else:\n",
    "            conversation.append(f\"Contenu ({role}):\")\n",
    "            conversation.append(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    \n",
    "    conversation.append(f\"\\n{'=' * 80}\")\n",
    "    conversation.append(f\"üìä R√âSUM√â: {len(data)} messages analys√©s\")\n",
    "    conversation.append(f\"{'=' * 80}\")\n",
    "    \n",
    "    return \"\\n\".join(conversation)\n",
    "\n",
    "def format_system_content(content):\n",
    "    \"\"\"Formate le contenu syst√®me de mani√®re plus lisible\"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    formatted = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('##'):\n",
    "            formatted.append(f\"\\nüéØ {line[2:].strip()}\")\n",
    "        elif line.startswith('-'):\n",
    "            formatted.append(f\"  ‚Ä¢ {line[1:].strip()}\")\n",
    "        elif line and not line.startswith('#'):\n",
    "            formatted.append(f\"   {line}\")\n",
    "    \n",
    "    return \"\\n\".join(formatted) if formatted else content[:300] + \"...\"\n",
    "\n",
    "def format_user_content(content):\n",
    "    \"\"\"Formate le contenu utilisateur\"\"\"\n",
    "    if \"JSON:\" in content:\n",
    "        parts = content.split(\"JSON:\")\n",
    "        if len(parts) == 2:\n",
    "            intro = parts[0].strip()\n",
    "            json_part = parts[1].strip()\n",
    "            \n",
    "            formatted = []\n",
    "            if intro:\n",
    "                formatted.append(f\"üìù {intro}\")\n",
    "            \n",
    "            formatted.append(\"\\nüìÑ Donn√©es JSON fournies:\")\n",
    "            try:\n",
    "                parsed_json = json.loads(json_part)\n",
    "                formatted.append(format_json_nicely(parsed_json))\n",
    "            except:\n",
    "                formatted.append(json_part[:200] + \"...\" if len(json_part) > 200 else json_part)\n",
    "            \n",
    "            return \"\\n\".join(formatted)\n",
    "    \n",
    "    return content[:300] + \"...\" if len(content) > 300 else content\n",
    "\n",
    "def format_assistant_content(content):\n",
    "    \"\"\"Formate le contenu de l'assistant\"\"\"\n",
    "    if \"SQL\" in content.upper():\n",
    "        return f\"üíæ Code SQL g√©n√©r√©:\\n{content}\"\n",
    "    return content[:400] + \"...\" if len(content) > 400 else content\n",
    "\n",
    "def format_ipython_content(content):\n",
    "    \"\"\"Formate le contenu d'ex√©cution Python/SQL\"\"\"\n",
    "    if \"Error\" in content:\n",
    "        return f\"‚ùå {content}\"\n",
    "    elif \"Success\" in content:\n",
    "        return f\"‚úÖ {content}\"\n",
    "    else:\n",
    "        return f\"üìã {content}\"\n",
    "\n",
    "def format_json_nicely(json_obj, indent=2):\n",
    "    \"\"\"Formate un objet JSON de mani√®re lisible\"\"\"\n",
    "    if isinstance(json_obj, dict):\n",
    "        formatted = []\n",
    "        for key, value in json_obj.items():\n",
    "            if isinstance(value, (dict, list)):\n",
    "                formatted.append(f\"  {key}: {type(value).__name__} avec {len(value)} √©l√©ments\")\n",
    "            else:\n",
    "                formatted.append(f\"  {key}: {value}\")\n",
    "        return \"\\n\".join(formatted)\n",
    "    \n",
    "    return json.dumps(json_obj, indent=indent, ensure_ascii=False)\n",
    "\n",
    "# Fonction utilitaire pour analyser directement un fichier\n",
    "def analyze_conversation_file(file_path, save_output=False, output_path=None):\n",
    "    \"\"\"\n",
    "    Analyse un fichier JSON de conversation et optionnellement sauvegarde le r√©sultat\n",
    "    \n",
    "    Args:\n",
    "        file_path: chemin vers le fichier JSON\n",
    "        save_output: si True, sauvegarde le r√©sultat dans un fichier texte\n",
    "        output_path: chemin de sortie (optionnel, sinon utilise le nom du fichier d'entr√©e)\n",
    "    \n",
    "    Returns:\n",
    "        String format√© de la conversation\n",
    "    \"\"\"\n",
    "    result = parse_conversation(file_path)\n",
    "    \n",
    "    if save_output:\n",
    "        if output_path is None:\n",
    "            # G√©n√®re un nom de fichier de sortie bas√© sur l'entr√©e\n",
    "            base_name = file_path.replace('.json', '').replace('\\\\', '/').split('/')[-1]\n",
    "            output_path = f\"{base_name}_analyzed.txt\"\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(result)\n",
    "            print(f\"‚úÖ Analyse sauvegard√©e dans: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors de la sauvegarde: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    result = parse_conversation('conversation_history.json')\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa563ee0-b819-42fb-9dee-457a54c35e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
