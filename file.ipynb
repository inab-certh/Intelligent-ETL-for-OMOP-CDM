{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3106933-22c4-4240-a01a-51ca901bcfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "def execute_sql_script_from_file(file_path, user=\"admin\", password=\"adminpassword\", host=\"localhost\", port=\"5432\"):\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        return [False, f\"SQL file not found: {file_path}\"]\n",
    "    \n",
    "    dbname = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_script = f.read()\n",
    "\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=dbname,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            host=host,\n",
    "            port=port\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        print(f\"Executing SQL script for database '{dbname}' from file: {file_path}\")\n",
    "        cursor.execute(sql_script)\n",
    "\n",
    "        if cursor.description:\n",
    "            colnames = [desc[0] for desc in cursor.description]\n",
    "            rows = cursor.fetchall()\n",
    "            df = pd.DataFrame(rows, columns=colnames)\n",
    "            return [True, df]\n",
    "        \n",
    "        return [True, None]\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        error_msg = f\"[{dbname}] SQL Execution Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return [False, error_msg]\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"[{dbname}] Unexpected Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return [False, error_msg]\n",
    "\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def groq_chat_completion_stream_clean(prompt, model=\"llama3-8b-8192\"):\n",
    "    GROQ_API_KEY = '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'\n",
    "    if not GROQ_API_KEY:\n",
    "        raise ValueError(\"La clé API Groq n'est pas configurée dans le fichier .env\")\n",
    "    \n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": True  \n",
    "    }\n",
    "    response_text = \"\"\n",
    "    with requests.post(url, headers=headers, json=data, stream=True) as response:\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Erreur API: {response.status_code} - {response.text}\")\n",
    "        \n",
    "        for chunk in response.iter_lines():\n",
    "            if chunk:\n",
    "                decoded_chunk = chunk.decode('utf-8')\n",
    "                if decoded_chunk.startswith(\"data:\"):\n",
    "                    try:\n",
    "                        parsed = json.loads(decoded_chunk[5:].strip())\n",
    "                        content = parsed.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\")\n",
    "                        if content:\n",
    "                            response_text += content\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  \n",
    "\n",
    "    return response_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c3458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1760e-b7e1-4053-9136-9e892ce5d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"/json/location/\"\n",
    "conversation_history = []\n",
    "\n",
    "with open(\"prompt/prompt1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "def extract_and_append_sql(text, json_data, sql_file_path):\n",
    "    sql_blocks = re.findall(r\"```sql\\s+(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if not sql_blocks:\n",
    "        return\n",
    "\n",
    "    step_id = json_data.get(\"id\", \"unknown\")\n",
    "\n",
    "    with open(sql_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for block in sql_blocks:\n",
    "            f.write(f\"-- BEGIN STEP: {step_id}\\n\")\n",
    "            f.write(block.strip() + \"\\n\")\n",
    "            f.write(f\"-- END STEP: {step_id}\\n\\n\")\n",
    "\n",
    "    print(f\"Appended {len(sql_blocks)} SQL block(s) for STEP {step_id} to {sql_file_path}\")\n",
    "\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt\n",
    "}\n",
    "\n",
    "def edit_sql_file(id: int, sql_script: str, file_named: str):\n",
    "    begin_marker = f\"-- BEGIN STEP: {id}\"\n",
    "    end_marker = f\"-- END STEP: {id}\"\n",
    "\n",
    "    with open(file_named, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    start_index = None\n",
    "    end_index = None\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == begin_marker:\n",
    "            start_index = i\n",
    "        elif line.strip() == end_marker:\n",
    "            end_index = i\n",
    "            break\n",
    "\n",
    "    if start_index is None or end_index is None or start_index >= end_index:\n",
    "        raise ValueError(f\"STEP with id {id} not found in the file.\")\n",
    "\n",
    "    # Replace the lines between the markers with the new SQL script\n",
    "    new_script_lines = [begin_marker + '\\n'] + [line + '\\n' for line in sql_script.strip().split('\\n')] + [end_marker + '\\n']\n",
    "    lines = lines[:start_index] + new_script_lines + lines[end_index + 1:]\n",
    "\n",
    "    with open(file_named, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "def contains_task_completed(text):\n",
    "    return re.search(r\"task\\s+completed\", text, re.IGNORECASE)\n",
    "\n",
    "conversation_history.append(system_message)\n",
    "\n",
    "for idx, file_name in enumerate(sorted(os.listdir(folder))):\n",
    "    path = os.path.join(folder, file_name)\n",
    "    if not os.path.isfile(path):\n",
    "        continue\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    json_str = json.dumps(json_data, indent=2)\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": json_str\n",
    "    })\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Here is the JSON:\\n\\n{json_str}\"\n",
    "    })\n",
    "\n",
    "    full_context = \"\\n\\n\".join([m[\"content\"] for m in conversation_history])\n",
    "    llm_response = groq_chat_completion_stream_clean(full_context)\n",
    "\n",
    "    extract_and_append_sql(llm_response, json_data, \"SQL/location.sql\")\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": llm_response\n",
    "    })\n",
    "\n",
    "with open(\"conversation_history.json\", \"w\") as f:\n",
    "    json.dump(conversation_history, f, indent=2)\n",
    "\n",
    "\n",
    "while True:\n",
    "    result = execute_sql_script_from_file(\"SQL/location.sql\")\n",
    "\n",
    "    if result[0]:\n",
    "        conversation_history.append({\n",
    "            \"role\": \"ipython\",\n",
    "            \"content\": \"# ✅ SQL executed successfully.\" + result[1]\n",
    "        })\n",
    "    else:\n",
    "        error_msg = f\"# ❌ SQL Execution Error:\\n{result[1]}\"\n",
    "        conversation_history.append({\n",
    "            \"role\": \"ipython\",\n",
    "            \"content\": error_msg\n",
    "        })\n",
    "\n",
    "    full_context = \"\\n\\n\".join([m[\"content\"] for m in conversation_history])\n",
    "    llm_response = groq_chat_completion_stream_clean(full_context)\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": llm_response\n",
    "    })\n",
    "\n",
    "    with open(\"SQL/location.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "        sql_blocks = re.findall(r\"```sql\\s+(.*?)```\", llm_response, re.DOTALL | re.IGNORECASE)\n",
    "        for block in sql_blocks:\n",
    "            f.write(block.strip() + \"\\n\\n\")\n",
    "\n",
    "    if contains_task_completed(llm_response):\n",
    "        print(\"✅ Task completed.\")\n",
    "        break\n",
    "\n",
    "with open(\"conversation_history.json\", \"w\") as f:\n",
    "    json.dump(conversation_history, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessed {len(conversation_history)} messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b48c57-6ecd-473e-bf68-4c365c40983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def update_sql_file(filename: str, step_id: int, sql_block: str):\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    pattern = re.compile(\n",
    "        rf'-- BEGIN STEP: {step_id}\\n.*?-- END STEP: {step_id}',\n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    new_block = f'-- BEGIN STEP: {step_id}\\n{sql_block.strip()}\\n-- END STEP: {step_id}'\n",
    "\n",
    "    if pattern.search(content):\n",
    "        updated = pattern.sub(new_block, content)\n",
    "    else:\n",
    "        # Append if the step doesn't exist\n",
    "        updated = f'{content.strip()}\\n\\n{new_block}'\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ca6776-d013-4baf-9742-184c83d3a147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the generated SQL code:\n",
      "\n",
      "```\n",
      "INSERT INTO omop.location (\n",
      "    city,\n",
      "    state,\n",
      "    zip,\n",
      "    location_source_value,\n",
      "    address_1,\n",
      "    address_2,\n",
      "    county\n",
      ")\n",
      "SELECT \n",
      "    l.city,\n",
      "    l.state_abbreviation AS state,\n",
      "    l.zip,\n",
      "    l.zip AS location_source_value,\n",
      "    NULL AS address_1, -- since column-level transformation rule is to null\n",
      "    NULL AS address_2, -- since column-level transformation rule is to null\n",
      "    NULL AS county   -- since column-level transformation rule is to null\n",
      "FROM \n",
      "    location_enriched_view AS l;\n",
      "```\n",
      "\n",
      "Note: I've included inline comments to clarify the logic of column-level transformation rules that result in `NULL` values. I've also assumed that the `cast_type` values for `address_1`, `address_2`, and `county` are not necessary since they don't seem to have a specific transformation rule. If there's an error in the execution, I'd be happy to review and correct!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"prompt/prompt1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "try:\n",
    "\n",
    "    with open(\"json/location/location_test_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    json_str = json.dumps(json_data, indent=2)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"JSON file not found. Please check the file path.\")\n",
    "    json_str = \"{}\"\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Invalid JSON format: {e}\")\n",
    "    json_str = \"{}\"\n",
    "\n",
    "final_prompt = prompt + \"\\n\\n\" + json_str\n",
    "result = groq_chat_completion_stream_clean(final_prompt)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ee806-e07a-4db8-9c26-b1984501571a",
   "metadata": {},
   "source": [
    "Here is the generated SQL code:\n",
    "\n",
    "```\n",
    "CREATE OR REPLACE VIEW omop.location_enriched_view AS \n",
    "SELECT \n",
    "  s.city, \n",
    "  s.state, \n",
    "  s.zip, \n",
    "  sm.state_abbreviation\n",
    "FROM \n",
    "  synthea.patients s \n",
    "  LEFT JOIN omop.states_map ON s.state = sm.state;\n",
    "```\n",
    "```\n",
    "INSERT INTO omop.location (\n",
    "  location_id,\n",
    "  city,\n",
    "  state,\n",
    "  zip,\n",
    "  location_source_value,\n",
    "  address_1,\n",
    "  address_2,\n",
    "  county\n",
    ")\n",
    "SELECT \n",
    "  MD5HASH(city || state_abbreviation || zip)::uuid AS location_id,\n",
    "  city,\n",
    "  state_abbreviation AS state,\n",
    "  zip,\n",
    "  zip AS location_source_value,\n",
    "  NULL::VARCHAR AS address_1,\n",
    "  NULL::VARCHAR AS address_2,\n",
    "  NULL::VARCHAR AS county\n",
    "FROM \n",
    "  location_enriched_view;\n",
    "```\n",
    "Let me know if this generates any execution errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e721ce7-3259-4931-a394-35c2e09d7ea0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
