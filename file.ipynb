{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86025644-b9d1-4718-973b-7660d29649a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_script_from_file(file_path, dbname=\"omop\", user=\"admin\", password=\"adminpassword\", host=\"localhost\", port=\"5432\"):\n",
    "    if not os.path.isfile(file_path):\n",
    "        return [False, f\"SQL file not found: {file_path}\"]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_script = f.read()\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=dbname,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            host=host,\n",
    "            port=port\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Executing SQL script for database '{dbname}' from file: {file_path}\")\n",
    "        cursor.execute(sql_script)\n",
    "        if cursor.description:\n",
    "            colnames = [desc[0] for desc in cursor.description]\n",
    "            rows = cursor.fetchall()\n",
    "            df = pd.DataFrame(rows, columns=colnames)\n",
    "            return [True, df.to_string()]\n",
    "        \n",
    "        return [True, \"SQL executed successfully with no results to display.\"]\n",
    "    except Exception as e:\n",
    "        return [False, f\"SQL Execution Error: {str(e)}\"]\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        error_msg = f\"[{dbname}] SQL Execution Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return [False, error_msg]\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"[{dbname}] Unexpected Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return [False, error_msg]\n",
    "\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def execute_sql_script_cross_schema(file_path, target_db=\"omop\", user=\"admin\", password=\"adminpassword\", host=\"localhost\", port=\"5432\"):\n",
    "    if not os.path.isfile(file_path):\n",
    "        return [False, f\"SQL file not found: {file_path}\"]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sql_script = f.read()\n",
    "        \n",
    "        # Se connecter directement à la base qui contient tous les schémas\n",
    "        # (assumant que synthea et omop sont des schémas dans la même base)\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=target_db,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            host=host,\n",
    "            port=port\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Définir le search_path pour inclure les deux schémas\n",
    "        cursor.execute(\"SET search_path TO omop, synthea, public;\")\n",
    "        \n",
    "        print(f\"Executing cross-schema SQL script on database '{target_db}' from file: {file_path}\")\n",
    "        cursor.execute(sql_script)\n",
    "        \n",
    "        if cursor.description:\n",
    "            colnames = [desc[0] for desc in cursor.description]\n",
    "            rows = cursor.fetchall()\n",
    "            df = pd.DataFrame(rows, columns=colnames)\n",
    "            return [True, df.to_string()]\n",
    "        \n",
    "        return [True, \"SQL executed successfully with no results to display.\"]\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "        error_msg = f\"[{target_db}] SQL Execution Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return [False, error_msg]\n",
    "    except Exception as e:\n",
    "        error_msg = f\"[{target_db}] Unexpected Error: {e}\"\n",
    "        print(error_msg)\n",
    "        return [False, error_msg]\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def groq_chat_completion_stream_clean(prompt, model=\"llama3-70b-8192\"):\n",
    "    GROQ_API_KEY = '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'\n",
    "    if not GROQ_API_KEY:\n",
    "        raise ValueError(\"La clé API Groq n'est pas configurée dans le fichier .env\")\n",
    "\n",
    "    time.sleep(4)\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": True  \n",
    "    }\n",
    "    response_text = \"\"\n",
    "    with requests.post(url, headers=headers, json=data, stream=True) as response:\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Erreur API: {response.status_code} - {response.text}\")\n",
    "        \n",
    "        for chunk in response.iter_lines():\n",
    "            if chunk:\n",
    "                decoded_chunk = chunk.decode('utf-8')\n",
    "                if decoded_chunk.startswith(\"data:\"):\n",
    "                    try:\n",
    "                        parsed = json.loads(decoded_chunk[5:].strip())\n",
    "                        content = parsed.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\")\n",
    "                        if content:\n",
    "                            response_text += content\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  \n",
    "\n",
    "    return response_text.strip()\n",
    "\n",
    "def extract_and_append_sql(text, json_data, sql_file_path):\n",
    "    # Pattern corrigé : \\s* au lieu de \\s+ pour accepter zéro ou plusieurs espaces/nouvelles lignes\n",
    "    sql_blocks = re.findall(r\"```sql\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Si le premier pattern ne fonctionne pas, essayer sans 'sql'\n",
    "    if not sql_blocks:\n",
    "        sql_blocks = re.findall(r\"```\\s*sql\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Dernier essai avec n'importe quelles balises ```\n",
    "    if not sql_blocks:\n",
    "        sql_blocks = re.findall(r\"```[a-zA-Z]*\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if not sql_blocks:\n",
    "        print(f\"❌ Aucun bloc SQL trouvé pour l'étape {json_data.get('id', 'unknown')}\")\n",
    "        print(f\"Texte analysé (premiers 200 chars): {text[:200]}\")\n",
    "        return\n",
    "    \n",
    "    step_id = json_data.get(\"id\", \"unknown\")\n",
    "    \n",
    "    # Créer le dossier s'il n'existe pas\n",
    "    os.makedirs(os.path.dirname(sql_file_path), exist_ok=True)\n",
    "    \n",
    "    with open(sql_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for block in sql_blocks:\n",
    "            f.write(f\"-- BEGIN STEP: {step_id}\\n\")\n",
    "            f.write(block.strip() + \"\\n\")\n",
    "            f.write(f\"-- END STEP: {step_id}\\n\\n\")\n",
    "    \n",
    "    print(f\"✅ Ajouté {len(sql_blocks)} bloc(s) SQL pour l'étape {step_id} dans {sql_file_path}\")\n",
    "\n",
    "def edit_sql_file(id: int, sql_script: str, file_named: str):\n",
    "    begin_marker = f\"-- BEGIN STEP: {id}\"\n",
    "    end_marker = f\"-- END STEP: {id}\"\n",
    "\n",
    "    with open(file_named, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    start_index = None\n",
    "    end_index = None\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == begin_marker:\n",
    "            start_index = i\n",
    "        elif line.strip() == end_marker:\n",
    "            end_index = i\n",
    "            break\n",
    "\n",
    "    if start_index is None or end_index is None or start_index >= end_index:\n",
    "        raise ValueError(f\"STEP with id {id} not found in the file.\")\n",
    "\n",
    "    # Replace the lines between the markers with the new SQL script\n",
    "    new_script_lines = [begin_marker + '\\n'] + [line + '\\n' for line in sql_script.strip().split('\\n')] + [end_marker + '\\n']\n",
    "    lines = lines[:start_index] + new_script_lines + lines[end_index + 1:]\n",
    "\n",
    "    with open(file_named, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "def contains_task_completed(text):\n",
    "    return re.search(r\"task\\s+completed\", text, re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3106933-22c4-4240-a01a-51ca901bcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ajouté 1 bloc(s) SQL pour l'étape 0 dans SQL/location.sql\n",
      "✅ Ajouté 1 bloc(s) SQL pour l'étape 1 dans SQL/location.sql\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os \n",
    "import time \n",
    "\n",
    "folder = \"json/location/\"\n",
    "conversation_history = []\n",
    "\n",
    "with open(\"prompt/prompt1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": prompt\n",
    "}\n",
    "\n",
    "conversation_history.append(system_message)\n",
    "\n",
    "for idx, file_name in enumerate(sorted(os.listdir(folder))):\n",
    "    path = os.path.join(folder, file_name)\n",
    "    if not os.path.isfile(path):\n",
    "        continue\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    json_str = json.dumps(json_data, indent=2)\n",
    "\n",
    "    # FIX: Only add JSON once, not twice\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Here is the JSON:\\n\\n{json_str}\"\n",
    "    })\n",
    "\n",
    "    full_context = \"\\n\\n\".join([m[\"content\"] for m in conversation_history])\n",
    "    llm_response = groq_chat_completion_stream_clean(full_context)\n",
    "\n",
    "    extract_and_append_sql(llm_response, json_data, \"SQL/location.sql\")\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": llm_response\n",
    "    })\n",
    "\n",
    "with open(\"conversation_history.json\", \"w\") as f:\n",
    "    json.dump(conversation_history, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "239d3f50-f620-4c62-b495-f2b74cf5c598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SQL script for database 'omop' from file: SQL/location.sql\n",
      "Executing SQL script for database 'omop' from file: SQL/location.sql\n",
      "Executing SQL script for database 'omop' from file: SQL/location.sql\n",
      "\n",
      "Processed 11 messages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i=0\n",
    "\n",
    "while i < 3 :\n",
    "    \n",
    "    result = execute_sql_script_from_file(\"SQL/location.sql\", dbname=\"omop\")\n",
    "\n",
    "    if result[0]:\n",
    "        success_msg = f\"# ✅ SQL executed successfully.\\n\\n{result[1]}\" \n",
    "        conversation_history.append({\n",
    "            \"role\": \"ipython\",\n",
    "            \"content\": success_msg\n",
    "        })\n",
    "    else:\n",
    "        error_msg = f\"# ❌ SQL Execution Error:\\n{result[1]}\"\n",
    "        conversation_history.append({\n",
    "            \"role\": \"ipython\",\n",
    "            \"content\": error_msg\n",
    "        })\n",
    "\n",
    "    full_context = \"\\n\\n\".join([m[\"content\"] for m in conversation_history])\n",
    "    llm_response = groq_chat_completion_stream_clean(full_context)\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": llm_response\n",
    "    })\n",
    "    \n",
    "    pattern = r'<function=([a-zA-Z_]+)>({.*?})<\\/function>'\n",
    "    \n",
    "    # FIX: Process function calls properly\n",
    "    function_calls_found = False\n",
    "    for match in re.finditer(pattern, llm_response, re.DOTALL):\n",
    "        function_calls_found = True\n",
    "        function_name = match.group(1)\n",
    "        function_args_raw = match.group(2)\n",
    "        \n",
    "        try:\n",
    "            function_args = json.loads(function_args_raw)\n",
    "            \n",
    "            if function_name == \"edit_sql_file\":\n",
    "                sql_script = function_args.get(\"sql_script\")\n",
    "                id = function_args.get(\"id\")\n",
    "                file_named = function_args.get(\"file_named\")\n",
    "                edit_sql_file(id, sql_script, file_named)\n",
    "                print(f\"Edited SQL for step {id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing function call: {e}\")\n",
    "        \n",
    "    if contains_task_completed(llm_response):\n",
    "        print(\"✅ Task completed.\")\n",
    "        break\n",
    "    i = i +1 \n",
    "\n",
    "with open(\"conversation_history.json\", \"w\") as f:\n",
    "    json.dump(conversation_history, f, indent=2)\n",
    "\n",
    "print(f\"\\nProcessed {len(conversation_history)} messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfa544ef-9c6c-4e55-8c1d-8f9ebd02ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**You are a highly capable AI specialized in generating optimized SQL code.**\n",
      "\n",
      "## Objective\n",
      "\n",
      "Generate SQL code that transforms data from a **source database (A)** into a **target database (B)** named `omop`.\n",
      "Each input will include one or more JSON objects describing the transformation logic for a **specific target table**.\n",
      "\n",
      "## Input Details\n",
      "\n",
      "Each JSON input contains:\n",
      "\n",
      "* The structure of the source table(s) involved\n",
      "* Column-level transformation rules\n",
      "* Optional metadata (e.g., view names, output types, primary-key logic)\n",
      "* **A unique `id` identifying the JSON object**\n",
      "\n",
      "## Constraints\n",
      "\n",
      "* Generate SQL **only** for the target table defined in the JSON.\n",
      "* Do **not** infer or generate code for other target tables.\n",
      "* All SQL must be limited to transforming data for the **one specified target table**.\n",
      "\n",
      "## Instructions\n",
      "\n",
      "* Analyze the input JSON carefully to understand the transformation logic.\n",
      "* Generate **clean, well-structured, and readable PostgreSQL SQL code**.\n",
      "* Include **inline SQL comments** only when needed to explain non-obvious logic.\n",
      "* Do **not** include any narrative explanation or commentary in your output.\n",
      "* Wrap the code in a **SQL code fence**.\n",
      "\n",
      "## Editing an Existing Script\n",
      "\n",
      "If you need to update the SQL previously generated for a particular JSON object, use the `edit_sql_file` function **with its `id` parameter set to the same `id` found in that JSON**.\n",
      "\n",
      "## Available Function\n",
      "\n",
      "{\n",
      "  \"name\": \"edit_sql_file\",\n",
      "  \"description\": \"Edit or replace SQL script between specific STEP markers in a SQL file.\",\n",
      "  \"parameters\": {\n",
      "    \"id\": {\n",
      "      \"param_type\": \"int\",\n",
      "      \"description\": \"Must match the `id` of the JSON whose SQL you are modifying.\",\n",
      "      \"required\": true\n",
      "    },\n",
      "    \"sql_script\": {\n",
      "      \"param_type\": \"str\",\n",
      "      \"description\": \"SQL script in JSON string format (escaped newlines, no backticks).\",\n",
      "      \"required\": true\n",
      "    },\n",
      "    \"file_named\": {\n",
      "      \"param_type\": \"str\",\n",
      "      \"description\": \"Name of the SQL file to edit ( target_table.sql).\",\n",
      "      \"required\": true\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "### Function Call Format\n",
      "\n",
      "<function={function_name}>{parameters}</function>\n",
      "\n",
      "## Output\n",
      "\n",
      "Output **only** the final SQL code required to transform and insert data into the specified `omop` table.\n",
      "\n",
      "## Execution\n",
      "\n",
      "If execution errors occur, they will be returned via the IPython role for correction.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5241f2-98dc-4ca3-be1a-fb06d525c959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ca6776-d013-4baf-9742-184c83d3a147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the generated SQL code:\n",
      "\n",
      "```\n",
      "INSERT INTO omop.location (\n",
      "    city,\n",
      "    state,\n",
      "    zip,\n",
      "    location_source_value,\n",
      "    address_1,\n",
      "    address_2,\n",
      "    county\n",
      ")\n",
      "SELECT \n",
      "    l.city,\n",
      "    l.state_abbreviation AS state,\n",
      "    l.zip,\n",
      "    l.zip AS location_source_value,\n",
      "    NULL AS address_1, -- since column-level transformation rule is to null\n",
      "    NULL AS address_2, -- since column-level transformation rule is to null\n",
      "    NULL AS county   -- since column-level transformation rule is to null\n",
      "FROM \n",
      "    location_enriched_view AS l;\n",
      "```\n",
      "\n",
      "Note: I've included inline comments to clarify the logic of column-level transformation rules that result in `NULL` values. I've also assumed that the `cast_type` values for `address_1`, `address_2`, and `county` are not necessary since they don't seem to have a specific transformation rule. If there's an error in the execution, I'd be happy to review and correct!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"prompt/prompt1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "try:\n",
    "\n",
    "    with open(\"json/location/location_test_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    json_str = json.dumps(json_data, indent=2)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"JSON file not found. Please check the file path.\")\n",
    "    json_str = \"{}\"\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Invalid JSON format: {e}\")\n",
    "    json_str = \"{}\"\n",
    "\n",
    "final_prompt = prompt + \"\\n\\n\" + json_str\n",
    "result = groq_chat_completion_stream_clean(final_prompt)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ee806-e07a-4db8-9c26-b1984501571a",
   "metadata": {},
   "source": [
    "Here is the generated SQL code:\n",
    "\n",
    "```\n",
    "CREATE OR REPLACE VIEW omop.location_enriched_view AS \n",
    "SELECT \n",
    "  s.city, \n",
    "  s.state, \n",
    "  s.zip, \n",
    "  sm.state_abbreviation\n",
    "FROM \n",
    "  synthea.patients s \n",
    "  LEFT JOIN omop.states_map ON s.state = sm.state;\n",
    "```\n",
    "```\n",
    "INSERT INTO omop.location (\n",
    "  location_id,\n",
    "  city,\n",
    "  state,\n",
    "  zip,\n",
    "  location_source_value,\n",
    "  address_1,\n",
    "  address_2,\n",
    "  county\n",
    ")\n",
    "SELECT \n",
    "  MD5HASH(city || state_abbreviation || zip)::uuid AS location_id,\n",
    "  city,\n",
    "  state_abbreviation AS state,\n",
    "  zip,\n",
    "  zip AS location_source_value,\n",
    "  NULL::VARCHAR AS address_1,\n",
    "  NULL::VARCHAR AS address_2,\n",
    "  NULL::VARCHAR AS county\n",
    "FROM \n",
    "  location_enriched_view;\n",
    "```\n",
    "Let me know if this generates any execution errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2915c9df-a207-4c9b-8a84-a2e2a04e4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "📋 ANALYSE DE CONVERSATION\n",
      "================================================================================\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "🤖 SYSTÈME - Message 1\n",
      "------------------------------------------------------------\n",
      "📋 Instructions système:\n",
      "   You are a highly capable AI specialized in generating optimized SQL code.\n",
      "\n",
      "🎯 Objective:\n",
      "   Generate SQL code that transforms data from a **source database (A)** into a **target database (B)** named `omop`.\n",
      "   Each input will include one or more JSON objects describing the transformation logic for a **single specific target table**.\n",
      "\n",
      "🎯 Input Details:\n",
      "   Each JSON file will contain:\n",
      "  • The structure of the source table(s) involved\n",
      "  • Column-level transformation rules\n",
      "  • Optional metadata (e.g. view names, output type, primary key logic)\n",
      "\n",
      "🎯 Constraints:\n",
      "  • Only generate SQL for the specific target table defined in the JSON.\n",
      "  • Do not infer or process transformations for any other target tables.\n",
      "  • All SQL must be limited to transforming data for the **one target table** described.\n",
      "\n",
      "🎯 Instructions:\n",
      "  • Carefully analyze the input JSON to understand the transformation logic.\n",
      "  • Generate clean, well-structured, and **readable PostgreSQL SQL code**.\n",
      "  • Use the format: `INSERT INTO omop.target_table (...) SELECT ...`\n",
      "  • Include inline SQL comments **only** when necessary to clarify non-obvious logic.\n",
      "  • Do **not** include any narrative explanation or commentary in your output.\n",
      "\n",
      "🎯 Output:\n",
      "  • Output **only** the final SQL code needed to transform and insert data into the specified `omop` table.\n",
      "\n",
      "🎯 Execution:\n",
      "   All generated SQL code will be merged and tested after your completion.\n",
      "   If any execution errors occur, they will be returned via the IPython role for correction.\n",
      "\n",
      "------------------------------------------------------------\n",
      "👤 UTILISATEUR - Message 2\n",
      "------------------------------------------------------------\n",
      "💬 Demande utilisateur:\n",
      "📝 Here is the\n",
      "\n",
      "📄 Données JSON fournies:\n",
      "  id: 0\n",
      "  output_type: view\n",
      "  view_name: location_enriched_view\n",
      "  source_table: synthea.patients\n",
      "  joins: list avec 1 éléments\n",
      "  select: list avec 4 éléments\n",
      "\n",
      "------------------------------------------------------------\n",
      "🔧 ASSISTANT - Message 3\n",
      "------------------------------------------------------------\n",
      "🔧 Réponse assistant:\n",
      "💾 Code SQL généré:\n",
      "Here is the generated SQL code:\n",
      "\n",
      "-- \n",
      "INSERT INTO omop.location_enriched_view \n",
      "(\n",
      "    city, \n",
      "    state, \n",
      "    zip, \n",
      "    state_abbreviation\n",
      ")\n",
      "SELECT \n",
      "    p.city, \n",
      "    p.state, \n",
      "    p.zip, \n",
      "    sm.state_abbreviation\n",
      "FROM \n",
      "    synthea.patients p\n",
      "LEFT JOIN \n",
      "    omop.states_map sm \n",
      "ON \n",
      "    p.state = sm.state\n",
      "\n",
      "------------------------------------------------------------\n",
      "👤 UTILISATEUR - Message 4\n",
      "------------------------------------------------------------\n",
      "💬 Demande utilisateur:\n",
      "📝 Here is the\n",
      "\n",
      "📄 Données JSON fournies:\n",
      "  id: 1\n",
      "  target_table: omop.location\n",
      "  source_table: None\n",
      "  source_view: location_enriched_view\n",
      "  output_type: insert\n",
      "  primary_key: dict avec 4 éléments\n",
      "  columns: list avec 7 éléments\n",
      "\n",
      "------------------------------------------------------------\n",
      "🔧 ASSISTANT - Message 5\n",
      "------------------------------------------------------------\n",
      "🔧 Réponse assistant:\n",
      "💾 Code SQL généré:\n",
      "Here is the generated SQL code:\n",
      "\n",
      "INSERT INTO omop.location \n",
      "(\n",
      "    location_id, \n",
      "    city, \n",
      "    state, \n",
      "    zip, \n",
      "    location_source_value, \n",
      "    address_1, \n",
      "    address_2, \n",
      "    county\n",
      ")\n",
      "SELECT \n",
      "    MD5.Hash(MD5.Hash(l.city || l.state_abbreviation || l.zip))::text AS location_id, \n",
      "    l.city, \n",
      "    l.state, \n",
      "    l.zip, \n",
      "    l.zip, \n",
      "    NULL::VARCHAR, \n",
      "    NULL::VARCHAR, \n",
      "    NULL::VARCHAR\n",
      "FROM \n",
      "    (SELECT \n",
      "        city, \n",
      "        state, \n",
      "        zip, \n",
      "        state_abbreviation\n",
      "    FROM \n",
      "        location_enriched_view) l \n",
      "\n",
      "Here is the explanation:\n",
      "\n",
      "* The primary key is generated using a hash function based on the combination of city, state_abbreviation, and zip.\n",
      "* The columns city, state, zip, and location_source_value are mapped directly from the source columns.\n",
      "* The columns address_1, address_2, and county are set to NULL as per the provided transformation rules.\n",
      "\n",
      "------------------------------------------------------------\n",
      "⚡ EXÉCUTION - Message 6\n",
      "------------------------------------------------------------\n",
      "⚡ Résultat d'exécution:\n",
      "❌ # ❌ SQL Execution Error:\n",
      "[location] SQL Execution Error: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: FATAL:  database \"location\" does not exist\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "🔧 ASSISTANT - Message 7\n",
      "------------------------------------------------------------\n",
      "🔧 Réponse assistant:\n",
      "💾 Code SQL généré:\n",
      "I apologize for the error. It seems that the generated SQL code does not take into account the fact that the `omop.location` table may not exist in the target database. \n",
      "\n",
      "I will make sure to add a check for the existence of the table before executing the insertion query. If the table does not exist, it will create the table first.\n",
      "\n",
      "Here is the revised SQL code:\n",
      "\n",
      "```\n",
      "BEGIN\n",
      "    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name='location' AND table_schema='public') THEN\n",
      "        CREATE TABLE omop.location (\n",
      "            location_id VARCHAR(255) PRIMARY KEY,\n",
      "            city VARCHAR(255),\n",
      "            state VARCHAR(255),\n",
      "            zip VARCHAR(255),\n",
      "            location_source_value VARCHAR(255),\n",
      "            address_1 VARCHAR(255),\n",
      "            address_2 VARCHAR(255),\n",
      "            county VARCHAR(255)\n",
      "        );\n",
      "    END IF;\n",
      "\n",
      "    INSERT INTO omop.location \n",
      "    (\n",
      "        location_id, \n",
      "        city, \n",
      "        state, \n",
      "        zip, \n",
      "        location_source_value, \n",
      "        address_1, \n",
      "        address_2, \n",
      "        county\n",
      "    )\n",
      "    SELECT \n",
      "        MD5.Hash(MD5.Hash(l.city || l.state_abbreviation || l.zip))::text AS location_id, \n",
      "        l.city, \n",
      "        l.state, \n",
      "        l.zip, \n",
      "        l.zip, \n",
      "        NULL::VARCHAR, \n",
      "        NULL::VARCHAR, \n",
      "        NULL::VARCHAR\n",
      "    FROM \n",
      "        (SELECT \n",
      "            city, \n",
      "            state, \n",
      "            zip, \n",
      "            state_abbreviation\n",
      "        FROM \n",
      "            location_enriched_view) l \n",
      "END;\n",
      "```\n",
      "\n",
      "This revised SQL code will first check if the `omop.location` table exists. If it does not exist, it will create the table. If the table already exists, it will proceed with the insertion query.\n",
      "\n",
      "------------------------------------------------------------\n",
      "⚡ EXÉCUTION - Message 8\n",
      "------------------------------------------------------------\n",
      "⚡ Résultat d'exécution:\n",
      "❌ # ❌ SQL Execution Error:\n",
      "[location] SQL Execution Error: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: FATAL:  database \"location\" does not exist\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "🔧 ASSISTANT - Message 9\n",
      "------------------------------------------------------------\n",
      "🔧 Réponse assistant:\n",
      "💾 Code SQL généré:\n",
      "It seems you're encountering an error with the connection to the server. I'll revise the SQL code again to address this. It appears that the database \"location\" does not exist in the target database, which was not mentioned in the input JSON.\n",
      "\n",
      "------------------------------------------------------------\n",
      "⚡ EXÉCUTION - Message 10\n",
      "------------------------------------------------------------\n",
      "⚡ Résultat d'exécution:\n",
      "❌ # ❌ SQL Execution Error:\n",
      "[location] SQL Execution Error: connection to server at \"localhost\" (127.0.0.1), port 5432 failed: FATAL:  database \"location\" does not exist\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "🔧 ASSISTANT - Message 11\n",
      "------------------------------------------------------------\n",
      "🔧 Réponse assistant:\n",
      "💾 Code SQL généré:\n",
      "You're having some connection issues. Let me take another look at the JSON inputs again. Ah-ha! I see that the JSON doesn't specify the target database name, but rather the target table name. That's why we're getting a database \"location\" not found error. \n",
      "\n",
      "Let's revise the SQL code to use the omop database. I also noticed that the primary key generation logic might need some adjustments. Currently, it's generating a hash based on the combination of city, state_abbreviation, and zip. If this is indeed the intended logic, we might want to ensure that the same combination of values does not result in the same primary key. We can do this by adding a unique constraint on these columns.\n",
      "\n",
      "Here's the revised SQL code:\n",
      "\n",
      "```sql\n",
      "BEGIN\n",
      "    IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name='location' AND table_schema='public' AND table_catalog='omop') THEN\n",
      "        CREATE TABLE omop.location (\n",
      "            location_id VARCHAR(255) PRIMARY KEY,\n",
      "            city VARCHAR(255),\n",
      "            state VARCHAR(255),\n",
      "            zip VARCHAR(255),\n",
      "            location_source_value VARCHAR(255),\n",
      "            address_1 VARCHAR(255),\n",
      "            address_2 VARCHAR(255),\n",
      "            county VARCHAR(255),\n",
      "            UNIQUE (city, state, zip)\n",
      "        );\n",
      "    END IF;\n",
      "\n",
      "    INSERT INTO omop.location \n",
      "    (\n",
      "        location_id, \n",
      "        city, \n",
      "        state, \n",
      "        zip, \n",
      "        location_source_value, \n",
      "        address_1, \n",
      "        address_2, \n",
      "        county\n",
      "    )\n",
      "    SELECT \n",
      "        to_hex(replace(md5(replace('city ' || state_abbreviation || zip, ' ', '')), '/', '')::bytea)::varchar(255) AS location_id, \n",
      "        city, \n",
      "        state, \n",
      "        zip, \n",
      "        zip, \n",
      "        NULL::VARCHAR, \n",
      "        NULL::VARCHAR, \n",
      "        NULL::VARCHAR\n",
      "    FROM \n",
      "        (SELECT \n",
      "            city, \n",
      "            state, \n",
      "            zip, \n",
      "            state_abbreviation\n",
      "        FROM \n",
      "            location_enriched_view) l \n",
      "END;\n",
      "```\n",
      "\n",
      "This revised SQL code creates the `omop.location` table in the `omop` database, if it does not exist. It also sets the city, state, and zip columns as UNIQUE, ensuring that combinations of these values are not duplicated. This will avoid having the same primary key generated for different combinations.\n",
      "\n",
      "================================================================================\n",
      "📊 RÉSUMÉ: 11 messages analysés\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_conversation(json_input):\n",
    "    \"\"\"\n",
    "    Transforme une conversation JSON complexe en format lisible\n",
    "    \n",
    "    Args:\n",
    "        json_input: peut être:\n",
    "            - Un chemin vers un fichier JSON\n",
    "            - Un string JSON\n",
    "            - Un dict ou une liste de messages\n",
    "    \n",
    "    Returns:\n",
    "        String formaté de la conversation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Si c'est un chemin de fichier\n",
    "    if isinstance(json_input, str) and (json_input.endswith('.json') or '/' in json_input or '\\\\' in json_input):\n",
    "        try:\n",
    "            with open(json_input, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "        except FileNotFoundError:\n",
    "            return f\"❌ Erreur: Fichier '{json_input}' non trouvé\"\n",
    "        except json.JSONDecodeError as e:\n",
    "            return f\"❌ Erreur: JSON invalide dans le fichier '{json_input}'\\nDétail: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"❌ Erreur lors de la lecture du fichier '{json_input}': {e}\"\n",
    "    \n",
    "    # Si c'est un string JSON\n",
    "    elif isinstance(json_input, str):\n",
    "        try:\n",
    "            data = json.loads(json_input)\n",
    "        except json.JSONDecodeError:\n",
    "            return \"❌ Erreur: JSON invalide\"\n",
    "    else:\n",
    "        data = json_input\n",
    "    \n",
    "    # Si ce n'est pas une liste, on l'encapsule\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "    \n",
    "    # Mapping des rôles vers des symboles/couleurs\n",
    "    role_symbols = {\n",
    "        'system': '🤖 SYSTÈME',\n",
    "        'user': '👤 UTILISATEUR', \n",
    "        'assistant': '🔧 ASSISTANT',\n",
    "        'ipython': '⚡ EXÉCUTION'\n",
    "    }\n",
    "    \n",
    "    conversation = []\n",
    "    conversation.append(\"=\" * 80)\n",
    "    conversation.append(\"📋 ANALYSE DE CONVERSATION\")\n",
    "    conversation.append(\"=\" * 80)\n",
    "    conversation.append(\"\")\n",
    "    \n",
    "    for i, message in enumerate(data, 1):\n",
    "        if not isinstance(message, dict) or 'role' not in message:\n",
    "            conversation.append(f\"❌ Message {i}: Format invalide\")\n",
    "            continue\n",
    "            \n",
    "        role = message.get('role', 'unknown')\n",
    "        content = message.get('content', '[Pas de contenu]')\n",
    "        \n",
    "        # En-tête du message\n",
    "        symbol = role_symbols.get(role, f'❓ {role.upper()}')\n",
    "        conversation.append(f\"\\n{'-' * 60}\")\n",
    "        conversation.append(f\"{symbol} - Message {i}\")\n",
    "        conversation.append(f\"{'-' * 60}\")\n",
    "        \n",
    "        # Traitement spécialisé selon le rôle\n",
    "        if role == 'system':\n",
    "            conversation.append(\"📋 Instructions système:\")\n",
    "            conversation.append(format_system_content(content))\n",
    "            \n",
    "        elif role == 'user':\n",
    "            conversation.append(\"💬 Demande utilisateur:\")\n",
    "            conversation.append(format_user_content(content))\n",
    "            \n",
    "        elif role == 'assistant':\n",
    "            conversation.append(\"🔧 Réponse assistant:\")\n",
    "            conversation.append(format_assistant_content(content))\n",
    "            \n",
    "        elif role == 'ipython':\n",
    "            conversation.append(\"⚡ Résultat d'exécution:\")\n",
    "            conversation.append(format_ipython_content(content))\n",
    "            \n",
    "        else:\n",
    "            conversation.append(f\"Contenu ({role}):\")\n",
    "            conversation.append(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    \n",
    "    conversation.append(f\"\\n{'=' * 80}\")\n",
    "    conversation.append(f\"📊 RÉSUMÉ: {len(data)} messages analysés\")\n",
    "    conversation.append(f\"{'=' * 80}\")\n",
    "    \n",
    "    return \"\\n\".join(conversation)\n",
    "\n",
    "def format_system_content(content):\n",
    "    \"\"\"Formate le contenu système de manière plus lisible\"\"\"\n",
    "    lines = content.split('\\n')\n",
    "    formatted = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('##'):\n",
    "            formatted.append(f\"\\n🎯 {line[2:].strip()}\")\n",
    "        elif line.startswith('-'):\n",
    "            formatted.append(f\"  • {line[1:].strip()}\")\n",
    "        elif line and not line.startswith('#'):\n",
    "            formatted.append(f\"   {line}\")\n",
    "    \n",
    "    return \"\\n\".join(formatted) if formatted else content[:300] + \"...\"\n",
    "\n",
    "def format_user_content(content):\n",
    "    \"\"\"Formate le contenu utilisateur\"\"\"\n",
    "    if \"JSON:\" in content:\n",
    "        parts = content.split(\"JSON:\")\n",
    "        if len(parts) == 2:\n",
    "            intro = parts[0].strip()\n",
    "            json_part = parts[1].strip()\n",
    "            \n",
    "            formatted = []\n",
    "            if intro:\n",
    "                formatted.append(f\"📝 {intro}\")\n",
    "            \n",
    "            formatted.append(\"\\n📄 Données JSON fournies:\")\n",
    "            try:\n",
    "                parsed_json = json.loads(json_part)\n",
    "                formatted.append(format_json_nicely(parsed_json))\n",
    "            except:\n",
    "                formatted.append(json_part[:200] + \"...\" if len(json_part) > 200 else json_part)\n",
    "            \n",
    "            return \"\\n\".join(formatted)\n",
    "    \n",
    "    return content[:300] + \"...\" if len(content) > 300 else content\n",
    "\n",
    "def format_assistant_content(content):\n",
    "    \"\"\"Formate le contenu de l'assistant\"\"\"\n",
    "    if \"SQL\" in content.upper():\n",
    "        return f\"💾 Code SQL généré:\\n{content}\"\n",
    "    return content[:400] + \"...\" if len(content) > 400 else content\n",
    "\n",
    "def format_ipython_content(content):\n",
    "    \"\"\"Formate le contenu d'exécution Python/SQL\"\"\"\n",
    "    if \"Error\" in content:\n",
    "        return f\"❌ {content}\"\n",
    "    elif \"Success\" in content:\n",
    "        return f\"✅ {content}\"\n",
    "    else:\n",
    "        return f\"📋 {content}\"\n",
    "\n",
    "def format_json_nicely(json_obj, indent=2):\n",
    "    \"\"\"Formate un objet JSON de manière lisible\"\"\"\n",
    "    if isinstance(json_obj, dict):\n",
    "        formatted = []\n",
    "        for key, value in json_obj.items():\n",
    "            if isinstance(value, (dict, list)):\n",
    "                formatted.append(f\"  {key}: {type(value).__name__} avec {len(value)} éléments\")\n",
    "            else:\n",
    "                formatted.append(f\"  {key}: {value}\")\n",
    "        return \"\\n\".join(formatted)\n",
    "    \n",
    "    return json.dumps(json_obj, indent=indent, ensure_ascii=False)\n",
    "\n",
    "# Fonction utilitaire pour analyser directement un fichier\n",
    "def analyze_conversation_file(file_path, save_output=False, output_path=None):\n",
    "    \"\"\"\n",
    "    Analyse un fichier JSON de conversation et optionnellement sauvegarde le résultat\n",
    "    \n",
    "    Args:\n",
    "        file_path: chemin vers le fichier JSON\n",
    "        save_output: si True, sauvegarde le résultat dans un fichier texte\n",
    "        output_path: chemin de sortie (optionnel, sinon utilise le nom du fichier d'entrée)\n",
    "    \n",
    "    Returns:\n",
    "        String formaté de la conversation\n",
    "    \"\"\"\n",
    "    result = parse_conversation(file_path)\n",
    "    \n",
    "    if save_output:\n",
    "        if output_path is None:\n",
    "            # Génère un nom de fichier de sortie basé sur l'entrée\n",
    "            base_name = file_path.replace('.json', '').replace('\\\\', '/').split('/')[-1]\n",
    "            output_path = f\"{base_name}_analyzed.txt\"\n",
    "        \n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(result)\n",
    "            print(f\"✅ Analyse sauvegardée dans: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors de la sauvegarde: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    result = parse_conversation('conversation_history.json')\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa563ee0-b819-42fb-9dee-457a54c35e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
