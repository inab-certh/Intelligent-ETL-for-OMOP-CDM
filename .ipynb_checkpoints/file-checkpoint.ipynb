{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3106933-22c4-4240-a01a-51ca901bcfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def execute_sql_code_from_string(input_string):\n",
    "    sql_blocks = re.findall(r'```sql\\s*(.*?)\\s*```', input_string, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if not sql_blocks:\n",
    "        print(\"No SQL block found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"synthea\",\n",
    "            user=\"admin\",\n",
    "            password=\"adminpassword\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        for sql_code in sql_blocks:\n",
    "            print(f\"Executing SQL:\\n{sql_code}\\n---\")\n",
    "            cursor.execute(sql_code)\n",
    "\n",
    "            if cursor.description:\n",
    "                # Get column names\n",
    "                colnames = [desc[0] for desc in cursor.description]\n",
    "                # Fetch all data\n",
    "                rows = cursor.fetchall()\n",
    "                # Convert to DataFrame\n",
    "                df = pd.DataFrame(rows, columns=colnames)\n",
    "                return df\n",
    "\n",
    "        return None  \n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"SQL Execution Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "            \n",
    "def groq_chat_completion_stream_clean(prompt, model=\"llama3-8b-8192\"):\n",
    "    GROQ_API_KEY = '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'\n",
    "    if not GROQ_API_KEY:\n",
    "        raise ValueError(\"La clé API Groq n'est pas configurée dans le fichier .env\")\n",
    "    \n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": True  \n",
    "    }\n",
    "    response_text = \"\"\n",
    "    with requests.post(url, headers=headers, json=data, stream=True) as response:\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Erreur API: {response.status_code} - {response.text}\")\n",
    "        \n",
    "        for chunk in response.iter_lines():\n",
    "            if chunk:\n",
    "                decoded_chunk = chunk.decode('utf-8')\n",
    "                if decoded_chunk.startswith(\"data:\"):\n",
    "                    try:\n",
    "                        parsed = json.loads(decoded_chunk[5:].strip())\n",
    "                        content = parsed.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\")\n",
    "                        if content:\n",
    "                            response_text += content\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  \n",
    "\n",
    "    return response_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1ca6776-d013-4baf-9742-184c83d3a147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the optimized SQL code that transforms data from the source database (A) to the target database (B) according to the provided JSON:\n",
      "\n",
      "```sql\n",
      "-- Create a view to perform the transformation\n",
      "CREATE VIEW transformed_data AS\n",
      "SELECT \n",
      "    -- Use row_number to generate a unique location_id\n",
      "    ROW_NUMBER() OVER (PARTITION BY city, state, zip) AS location_id\n",
      "    , NULL AS address_1\n",
      "    , NULL AS address_2\n",
      "    , city\n",
      "    , -- Perform a lookup for state abbreviation\n",
      "    (SELECT state_abbreviation FROM omop.states_map WHERE state = patients.state) AS state\n",
      "    , NULL AS county\n",
      "    , zip\n",
      "    , zip AS location_source_value\n",
      "FROM \n",
      "    synthea.patients;\n",
      "\n",
      "-- Create the target table with the transformed data\n",
      "CREATE TABLE omop.location AS\n",
      "SELECT \n",
      "    location_id\n",
      "    , address_1\n",
      "    , address_2\n",
      "    , city\n",
      "    , state\n",
      "    , county\n",
      "    , zip\n",
      "    , location_source_value\n",
      "FROM \n",
      "    transformed_data;\n",
      "```\n",
      "\n",
      "Note:\n",
      "\n",
      "* I've created a view `transformed_data` to perform the transformation, which improves modularity and readability.\n",
      "* I've used `ROW_NUMBER()` to generate a unique `location_id` for each combination of `city`, `state`, and `zip`.\n",
      "* I've performed a lookup for `state` abbreviation using a subquery, as JOINS are not feasible in this scenario.\n",
      "* I've explicitly selected the required columns for the target table, avoiding `SELECT *`.\n",
      "* I've commented the SQL code for better understanding.\n",
      "\n",
      "Please let me know if there are any errors or issues with the generated code.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/home/petriscyril/Desktop/Agent_ETL/prompt/prompt2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "with open(\"location2.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "json_str = json.dumps(json_data, indent=2)\n",
    "\n",
    "final_prompt = prompt + \"\\n\\n\" + json_str\n",
    "\n",
    "result = groq_chat_completion_stream_clean(final_prompt)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582357fe-3355-4081-8996-a0d30701c6d4",
   "metadata": {},
   "source": [
    "prompt1 avec location 8\n",
    "\n",
    "Here is the generated SQL code:\n",
    "\n",
    "```sql\n",
    "CREATE VIEW omop.location_view AS\n",
    "WITH distinct_locations AS (\n",
    "    SELECT \n",
    "      ROW_NUMBER() OVER (PARTITION BY city, state, zip ORDER BY city, state, zip) AS location_id,\n",
    "      city,\n",
    "      s.state_abbreviation AS state, \n",
    "      zip,\n",
    "      zip AS location_source_value\n",
    "    FROM \n",
    "      synthea.patients p\n",
    "      LEFT JOIN omop.states_map s ON p.state = s.state\n",
    "  )\n",
    "SELECT \n",
    "  location_id,\n",
    "  city,\n",
    "  state,\n",
    "  zip,\n",
    "  location_source_value,\n",
    "  CAST(NULL AS VARCHAR) AS address_1,\n",
    "  CAST(NULL AS VARCHAR) AS address_2,\n",
    "  CAST(NULL AS VARCHAR) AS county\n",
    "FROM \n",
    "  distinct_locations;\n",
    "\n",
    "CREATE TABLE omop.location AS \n",
    "SELECT * FROM omop.location_view;\n",
    "```\n",
    "\n",
    "This SQL code generates a `VIEW` that performs the desired transformation, and then creates the `omop.location` table based on the view. Note that I've used a CTE (Common Table Expression) to compute the `location_id` and perform the `state` lookup. I've also explicitly cast the `NULL` values to `VARCHAR` type as specified in the JSON.\n",
    "\n",
    "prompt1 avec location llama8b mark = 8\n",
    "\n",
    "Here is the optimized SQL code that transforms the data from the source table \"synthea.patients\" to the target table \"omop.location\" according to the provided JSON:\n",
    "```sql\n",
    "-- Create a view to perform the distinct grouping and transformations\n",
    "CREATE VIEW location_transform AS\n",
    "SELECT \n",
    "  -- Use ROW_NUMBER() to generate a unique location_id\n",
    "  ROW_NUMBER() OVER (PARTITION BY p.city, p.state, p.zip) AS location_id,\n",
    "  p.city,\n",
    "  -- Perform the lookup on the omop.states_map table\n",
    "  sm.state_abbreviation AS state,\n",
    "  p.zip,\n",
    "  p.zip AS location_source_value,\n",
    "  -- Set address_1, address_2, and county to NULL\n",
    "  CAST(NULL AS VARCHAR) AS address_1,\n",
    "  CAST(NULL AS VARCHAR) AS address_2,\n",
    "  CAST(NULL AS VARCHAR) AS county\n",
    "FROM \n",
    "  synthea.patients p\n",
    "  -- Join with the omop.states_map table for lookup\n",
    "  LEFT JOIN omop.states_map ON p.state = sm.state;\n",
    "\n",
    "-- Create the target table omop.location from the view\n",
    "CREATE TABLE omop.location AS\n",
    "SELECT \n",
    "  *\n",
    "FROM \n",
    "  location_transform;\n",
    "```\n",
    "Note that I've followed the best practices to make the code efficient, readable, and maintainable. I've used a view to modularize the complex transformation and made sure to limit the selected columns. I've also used a join instead of a subquery for the lookup operation. Additionally, I've applied casting to ensure the correct data type for the address fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa54af-586e-4704-9683-098b916bf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from datetime import datetime\n",
    "import pytest\n",
    "\n",
    "def test_table_person_structure(docker_db):\n",
    "    expected_columns = {\n",
    "        'person_id': 'integer',\n",
    "        'gender_concept_id': 'integer',\n",
    "        'year_of_birth': 'integer',\n",
    "        'month_of_birth': 'integer',\n",
    "        'day_of_birth': 'integer',\n",
    "        'birth_datetime': 'timestamp without time zone', \n",
    "        'race_concept_id': 'integer',\n",
    "        'ethnicity_concept_id': 'integer',\n",
    "        'location_id': 'integer',\n",
    "        'provider_id': 'integer',\n",
    "        'care_site_id': 'integer',\n",
    "        'person_source_value': 'character varying',  \n",
    "        'gender_source_value': 'character varying',\n",
    "        'gender_source_concept_id': 'integer',\n",
    "        'race_source_value': 'character varying',\n",
    "        'race_source_concept_id': 'integer',\n",
    "        'ethnicity_source_value': 'character varying',\n",
    "        'ethnicity_source_concept_id': 'integer'\n",
    "    }\n",
    "    \n",
    "    id_columns = [\n",
    "        'person_id',\n",
    "        'gender_concept_id',\n",
    "        'race_concept_id',\n",
    "        'ethnicity_concept_id',\n",
    "        'location_id',\n",
    "        'provider_id',\n",
    "        'care_site_id',\n",
    "        'gender_source_concept_id',\n",
    "        'race_source_concept_id',\n",
    "        'ethnicity_source_concept_id'\n",
    "    ]\n",
    "    \n",
    "    with psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"omop\",\n",
    "        user=\"admin\",\n",
    "        password=\"adminpassword\",\n",
    "        port=\"5432\"\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT column_name, data_type\n",
    "                FROM information_schema.columns\n",
    "                WHERE table_name = 'person'\n",
    "                AND table_schema = 'public';  -- replace with your schema if not 'public'\n",
    "            \"\"\")\n",
    "            \n",
    "            actual_columns = {row[0]: row[1] for row in cursor.fetchall()}\n",
    "            \n",
    "            missing_columns = set(expected_columns.keys()) - set(actual_columns.keys())\n",
    "            assert not missing_columns, f\"Missing columns: {missing_columns}\"\n",
    "            \n",
    "            type_mismatches = []\n",
    "            for column, expected_type in expected_columns.items():\n",
    "                if column in actual_columns and actual_columns[column] != expected_type:\n",
    "                    type_mismatches.append(\n",
    "                        f\"{column}: expected {expected_type}, got {actual_columns[column]}\"\n",
    "                    )\n",
    "            \n",
    "            assert not type_mismatches, \"Type mismatches:\\n\" + \"\\n\".join(type_mismatches)\n",
    "            \n",
    "            # Check for NULL values in ID columns\n",
    "            null_check_issues = []\n",
    "            for id_column in id_columns:\n",
    "                if id_column in actual_columns:\n",
    "                    cursor.execute(f\"\"\"\n",
    "                        SELECT COUNT(*) \n",
    "                        FROM person \n",
    "                        WHERE {id_column} IS NULL;\n",
    "                    \"\"\")\n",
    "                    null_count = cursor.fetchone()[0]\n",
    "                    \n",
    "                    if null_count > 0:\n",
    "                        null_check_issues.append(\n",
    "                            f\"{id_column} has {null_count} NULL values\"\n",
    "                        )\n",
    "            \n",
    "            assert not null_check_issues, \"NULL value issues in ID columns:\\n\" + \"\\n\".join(null_check_issues)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "232c517d-b24a-417a-83e4-daef851adfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>gender_concept_id</th>\n",
       "      <th>year_of_birth</th>\n",
       "      <th>month_of_birth</th>\n",
       "      <th>day_of_birth</th>\n",
       "      <th>birth_datetime</th>\n",
       "      <th>race_concept_id</th>\n",
       "      <th>ethnicity_concept_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>provider_id</th>\n",
       "      <th>care_site_id</th>\n",
       "      <th>person_source_value</th>\n",
       "      <th>gender_source_value</th>\n",
       "      <th>gender_source_concept_id</th>\n",
       "      <th>race_source_value</th>\n",
       "      <th>race_source_concept_id</th>\n",
       "      <th>ethnicity_source_value</th>\n",
       "      <th>ethnicity_source_concept_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8532</td>\n",
       "      <td>2005</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2005-07-06</td>\n",
       "      <td>0</td>\n",
       "      <td>38003563</td>\n",
       "      <td>314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0002513e-8009-d8c4-9bf8-bdbb316deae8</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>native</td>\n",
       "      <td>0</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8532</td>\n",
       "      <td>2000</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>2000-10-18</td>\n",
       "      <td>8527</td>\n",
       "      <td>38003563</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00035f01-cb9a-d253-eb67-7007a4e19ded</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8532</td>\n",
       "      <td>2001</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>2001-09-25</td>\n",
       "      <td>8527</td>\n",
       "      <td>38003563</td>\n",
       "      <td>103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000b8952-a1f1-e576-834c-d55c9d7b0941</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8532</td>\n",
       "      <td>1989</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1989-10-01</td>\n",
       "      <td>8527</td>\n",
       "      <td>38003563</td>\n",
       "      <td>632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>001046c2-98bd-1b63-14c4-ab8f9a7ddfdc</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8532</td>\n",
       "      <td>1992</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1992-11-10</td>\n",
       "      <td>8527</td>\n",
       "      <td>38003563</td>\n",
       "      <td>456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0011424c-182d-59ac-5942-056a7f68d983</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>0</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_id  gender_concept_id  year_of_birth  month_of_birth  day_of_birth  \\\n",
       "0          1               8532           2005               7             6   \n",
       "1          2               8532           2000              10            18   \n",
       "2          3               8532           2001               9            25   \n",
       "3          4               8532           1989              10             1   \n",
       "4          5               8532           1992              11            10   \n",
       "\n",
       "  birth_datetime  race_concept_id  ethnicity_concept_id  location_id  \\\n",
       "0     2005-07-06                0              38003563          314   \n",
       "1     2000-10-18             8527              38003563           96   \n",
       "2     2001-09-25             8527              38003563          103   \n",
       "3     1989-10-01             8527              38003563          632   \n",
       "4     1992-11-10             8527              38003563          456   \n",
       "\n",
       "   provider_id  care_site_id                   person_source_value  \\\n",
       "0          NaN           NaN  0002513e-8009-d8c4-9bf8-bdbb316deae8   \n",
       "1          NaN           NaN  00035f01-cb9a-d253-eb67-7007a4e19ded   \n",
       "2          NaN           NaN  000b8952-a1f1-e576-834c-d55c9d7b0941   \n",
       "3          NaN           NaN  001046c2-98bd-1b63-14c4-ab8f9a7ddfdc   \n",
       "4          NaN           NaN  0011424c-182d-59ac-5942-056a7f68d983   \n",
       "\n",
       "  gender_source_value  gender_source_concept_id race_source_value  \\\n",
       "0                   F                         0            native   \n",
       "1                   F                         0             white   \n",
       "2                   F                         0             white   \n",
       "3                   F                         0             white   \n",
       "4                   F                         0             white   \n",
       "\n",
       "   race_source_concept_id ethnicity_source_value  ethnicity_source_concept_id  \n",
       "0                       0               hispanic                            0  \n",
       "1                       0               hispanic                            0  \n",
       "2                       0               hispanic                            0  \n",
       "3                       0               hispanic                            0  \n",
       "4                       0               hispanic                            0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person =  pd.read_csv('/home/petriscyril/Desktop/OMOP/OMOP_dataset/person.csv')\n",
    "person[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e167c8-f07f-405a-8bf3-7cefdaab6450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker-compose.yml  initdb.d  location2.json  prompt\t      test.py\n",
      "file.ipynb\t    init.sh   location.json   states_map.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa913198-fbf4-4b8e-8b66-591fdb5328eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
